{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.example_selectors.base import BaseExampleSelector\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "from src.dataset import Dataset\n",
    "\n",
    "from src.link_prediction.evaluation import Evaluator\n",
    "\n",
    "from src.utils import load_model\n",
    "\n",
    "from src.utils import format_paths\n",
    "from src.utils import read_json\n",
    "\n",
    "from src.utils import set_seeds\n",
    "\n",
    "from src import DATA_PATH, LP_CONFIGS_PATH\n",
    "\n",
    "from src import DB50K, DB100K, YAGO4_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pipeline():\n",
    "    model_id = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        batch_size=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        max_length=8192,\n",
    "        truncation=True\n",
    "    )\n",
    "    pipe.tokenizer.pad_token_id = model.config.eos_token_id[0]\n",
    "    pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = init_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Your response should be crisp, short and not repetitive.\n",
    "Discard any preamble, explanation, greeting, or final consideration.\n",
    "An RDF triple is a statement (subject, predicate, object).\n",
    "The subject and the object are entities, and the predicate is a relation between the subject and the object.\n",
    "Perform a Link Prediction (LP) task, specifically, given an incomplete RDF triple (subject, predicate, ?), predict the missing object that completes the triple and makes it a true statement.\n",
    "Strict requirement: output solely the name of a single object entity, discard any explanations or other text. \n",
    "Correct format: Elizabeth_of_Bohemia\n",
    "Incorrect format: The object entity is Elizabeth_of_Bohemia.\n",
    "{ranking}\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "({subject}, {predicate}, ?)\n",
    "{explanation}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "explanation_hook = \"\"\"\n",
    "In addition to the incomplete triple, an explanation is provided.\n",
    "An explanation is a set of RDF triples that provide context for the prediction.\n",
    "Given an incomplete triple (s, p, ?), all the RDF triples in the explanation feature s as subject or as object and connect it with another entity.\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "qhook = \"\"\"\n",
    "This explanation is also provided in the form of quotient triples to provide a more abstract view of the explanation.\n",
    "A quotient triple is an RDF triple featuring the subject entity of the incomplete triple as subject or as object and connects it with a concept name rather than an entity.\n",
    "Considering an example explanation with the following triples:\n",
    "(s, p, e)\n",
    "(s, p, f)\n",
    "(s, p, g)\n",
    "\n",
    "The corresponding quotient triples are:\n",
    "(s, p, C1)\n",
    "(s, p, C2)\n",
    "\n",
    "where C1, C2 are concept names, e and f are instances of C1, and g is an instance of C2.\n",
    "\n",
    "Your answer still must be a single entity name even if the quotient triples contain concept names.\n",
    "\n",
    "Finally, the quotient triples are:\n",
    "\"\"\"\n",
    "\n",
    "ranking_hook = \"\"\"\n",
    "The entity name that you provide must be in the following list of entity names:\n",
    "\"\"\"\n",
    "\n",
    "example_prompt_str_template = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "({subject}, {predicate}, ?)\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{object}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, dataset, df_output): \n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.df_output = df_output\n",
    "\n",
    "    def add_example(example):\n",
    "        return\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        predicate = input_variables[\"predicate\"]\n",
    "\n",
    "        examples = self.df_output[self.df_output[\"p\"] == predicate].copy()\n",
    "        examples = examples[examples[\"o_rank\"] == 1]\n",
    "        examples = examples.drop(columns=[\"s_rank\", \"o_rank\"])\n",
    "        examples = examples.rename(columns={\"s\": \"subject\", \"p\": \"predicate\", \"o\": \"object\"})\n",
    "\n",
    "        examples = examples.to_dict(\"records\")\n",
    "\n",
    "        return random.sample(examples, 10) if len(examples) > 10 else examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(explained_pred, summarization, e_sem):\n",
    "    if summarization == \"no\":\n",
    "        explanation = explained_pred[\"explanation\"]\n",
    "        explanation = [f\"({s}, {p}, {o})\\n\" for (s, p, o) in explanation]\n",
    "        explanation = \"\\n\".join(explanation)\n",
    "        explanation = f\"{explanation_hook}\\n{explanation}\"\n",
    "\n",
    "        return explanation\n",
    "    elif summarization == \"simulation\" or summarization == \"bisimulation\":\n",
    "        triples = explained_pred[\"explanation\"]\n",
    "        triples_string = [f\"({s}, {p}, {o})\" for (s, p, o) in triples]\n",
    "        triples_string = \"\\n\".join(triples_string)\n",
    "\n",
    "        pred_s = explained_pred[\"pred\"][0]\n",
    "\n",
    "        qtriples = explained_pred[\"quotient_explanation\"]\n",
    "        qtriples_string = []\n",
    "        for qtriple in qtriples:\n",
    "            s, p, o = qtriple\n",
    "            if len(s) == 1 and s[0] == pred_s:\n",
    "                s_ = s[0]\n",
    "            else:\n",
    "                if len(e_sem[s[0]]) == 0:\n",
    "                    e_sem[s[0]] = \"Thing\"\n",
    "                s_ = f\"[{e_sem[s[0]]}]\"\n",
    "            if len(o) == 1 and o[0] == pred_s:\n",
    "                o_ = o[0]\n",
    "            else:\n",
    "                if len(e_sem[o[0]]) == 0:\n",
    "                    e_sem[o[0]] = \"Thing\"\n",
    "                o_ = f\"[{e_sem[o[0]]}]\"\n",
    "\n",
    "            if s_ == o_:\n",
    "                s_ = pred_s\n",
    "            qtriples_string.append(f\"({s_}, {p}, {o_})\")\n",
    "        qtriples_string = \"\\n\".join(qtriples_string)\n",
    "\n",
    "        types = [f\"{s} is an instance of {e_sem[s]}\" for s, _, _ in triples if s != pred_s]\n",
    "        types += [f\"{o} is an instance of {e_sem[o]}\" for _, _, o in triples if o != pred_s]\n",
    "\n",
    "        types = \"\\n\".join(types)\n",
    "\n",
    "        explanation = f\"{explanation_hook}\\n{triples_string}\\n{qhook}\\n{qtriples_string}\\n{types}\"\n",
    "\n",
    "        return explanation\n",
    "\n",
    "def build_ranking(ranking):\n",
    "    ranking = [o for o in ranking]\n",
    "    ranking = \"\\n\".join(ranking)\n",
    "    ranking = f\"{ranking_hook}{ranking}\"\n",
    "\n",
    "    return ranking\n",
    "\n",
    "def parse(response):\n",
    "    substring = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    pos = response.rfind(substring) + len(substring)\n",
    "    response = response[pos:]\n",
    "    response = response.replace(\"\\n\", \"\")\n",
    "    response = response.replace(\" \", \"_\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"kelpie++\"\n",
    "mode = \"necessary\"\n",
    "model = \"TransE\"\n",
    "dataset = \"DB50K\"\n",
    "summarization = \"simulation\"\n",
    "include_ranking = True\n",
    "examples = False\n",
    "\n",
    "paths = format_paths(method, mode, model, dataset, summarization)\n",
    "\n",
    "lp_config_path = LP_CONFIGS_PATH / f\"{model}_{dataset}.json\"\n",
    "lp_config = read_json(lp_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset DB50K...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 11247 entities and 22 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 8871 from 10969 triples were filtered out\n",
      "You're trying to map triples with 332 entities and 1 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 276 from 399 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model TransE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset {dataset}...\")\n",
    "dataset = Dataset(dataset)\n",
    "print(f\"Loading model {model}...\")\n",
    "model = load_model(lp_config, dataset)\n",
    "model.eval()\n",
    "evaluator = Evaluator(model=model)\n",
    "ranks = evaluator.evaluate(triples=dataset.training_triples)\n",
    "df_output = evaluator.get_df_output(triples=dataset.training_triples, ranks=ranks)\n",
    "example_selector = CustomExampleSelector(dataset, df_output)\n",
    "example_prompt_template = PromptTemplate(\n",
    "    template=example_prompt_str_template, input_variables=[\"subject\", \"predicate\", \"object\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector if examples else None,\n",
    "    examples=None if examples else [],\n",
    "    example_prompt=example_prompt_template,\n",
    "    input_variables=[\"subject\", \"predicate\", \"explanation\", \"ranking\"],\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    example_separator=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = few_shot_template | pipe | parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = read_json(paths[\"rankings\"])\n",
    "rankings = {tuple(ranking[\"pred\"]): ranking[\"ranking\"][:100] for ranking in rankings}\n",
    "explained_preds = read_json(paths[\"exps\"])\n",
    "\n",
    "explained_preds = [\n",
    "    {\n",
    "        \"pred\": explained_pred[\"pred\"],\n",
    "        \"explanation\": explained_pred[\"explanation\"],\n",
    "        \"quotient_explanation\": explained_pred.get(\"quotient_explanation\", None),\n",
    "        \"label\": explained_pred.get(\"label\", None),\n",
    "        \"ranking\": rankings[tuple(explained_pred[\"pred\"])]\n",
    "    }\n",
    "    for explained_pred in explained_preds\n",
    "]\n",
    "\n",
    "# explained_preds = [ep for ep in explained_preds if ep[\"label\"] == 1]\n",
    "\n",
    "# random.shuffle(explained_preds)\n",
    "explained_preds = explained_preds[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        \"subject\": explained_pred[\"pred\"][0],\n",
    "        \"predicate\": explained_pred[\"pred\"][1],\n",
    "        \"explanation\": \"\",\n",
    "        \"ranking\": build_ranking(explained_pred[\"ranking\"]) if include_ranking else \"\",\n",
    "    }\n",
    "    for explained_pred in explained_preds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = [ep[\"pred\"][2] for ep in explained_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = chain.batch(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_sem = None\n",
    "\n",
    "if dataset.name in [DB50K, DB100K, YAGO4_20]:\n",
    "    e_sem = pd.read_csv(\n",
    "        DATA_PATH / dataset.name / \"reasoned\" / \"entities.csv\",\n",
    "        converters={\"classes\": literal_eval},\n",
    "    )\n",
    "    e_sem[\"classes\"] = e_sem[\"classes\"].map(sorted)\n",
    "    e_sem[\"classes\"] = e_sem[\"classes\"].map(lambda x: [c.split(\"/\")[-1] for c in x])\n",
    "    e_sem[\"classes\"] = e_sem[\"classes\"].map(\", \".join)\n",
    "    e_sem = e_sem.to_dict(\"records\")\n",
    "    e_sem = {e[\"entity\"]: e[\"classes\"] for e in e_sem}\n",
    "\n",
    "explained_queries = [\n",
    "    {\n",
    "        \"subject\": explained_pred[\"pred\"][0],\n",
    "        \"predicate\": explained_pred[\"pred\"][1],\n",
    "        \"explanation\": build_explanation(explained_pred, summarization, e_sem),\n",
    "        \"ranking\": build_ranking(explained_pred[\"ranking\"]) if include_ranking else \"\",\n",
    "    }\n",
    "    for explained_pred in explained_preds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Your response should be crisp, short and not repetitive.\n",
      "Discard any preamble, explanation, greeting, or final consideration.\n",
      "An RDF triple is a statement (subject, predicate, object).\n",
      "The subject and the object are entities, and the predicate is a relation between the subject and the object.\n",
      "Perform a Link Prediction (LP) task, specifically, given an incomplete RDF triple (subject, predicate, ?), predict the missing object that completes the triple and makes it a true statement.\n",
      "Strict requirement: output solely the name of a single object entity, discard any explanations or other text. \n",
      "Correct format: Elizabeth_of_Bohemia\n",
      "Incorrect format: The object entity is Elizabeth_of_Bohemia.\n",
      "\n",
      "The entity name that you provide must be in the following list of entity names:\n",
      "Swamp_Ophelia\n",
      "Rites_of_Passage_(Indigo_Girls_album)\n",
      "Hanna-Barbera's_All-Star_Comedy_Ice_Revue\n",
      "CBBC\n",
      "Pepe_Smith\n",
      "Brent_Fitz\n",
      "BBC_News_(TV_channel)\n",
      "BBC_Red_Button\n",
      "Mother_Love_Bone_(album)\n",
      "Iisa_Pa_Lamang\n",
      "Live_in_Europe_1967:_The_Bootleg_Series_Vol._1\n",
      "Screaming_Life/Fopp\n",
      "Alain_Johannes\n",
      "A_Flintstones_Christmas_Carol\n",
      "Muhammad:_The_Messenger_of_God_(film)\n",
      "Elliott_Smith\n",
      "Felix_Haug\n",
      "Phil_Redmond\n",
      "D-beat\n",
      "The_Five_Faces_of_Manfred_Mann\n",
      "It's_No_Good\n",
      "Samira_Koppikar\n",
      "Saki_Kaskas\n",
      "Utada_Hikaru_Single_Collection_Vol._2\n",
      "Mark_Dodson\n",
      "Toshio_Okada\n",
      "Graham_Lyle\n",
      "Angus_MacLise\n",
      "Gisele_MacKenzie\n",
      "King_Oliver\n",
      "Hans_Beimler_(screenwriter)\n",
      "Etiquette_for_Mistresses\n",
      "Andy_DeMize\n",
      "Juan_Carlos_Paz_y_Puente\n",
      "Herman_Matthews\n",
      "All_About_Eve_(Philippine_TV_series)\n",
      "Bates_Motel_(TV_series)\n",
      "Richard_Ploog\n",
      "Neil_Peart\n",
      "The_Barkleys\n",
      "Lucy_Craft_Laney\n",
      "Guns_and_Roses_(TV_series)\n",
      "Walter_Murphy\n",
      "Byker_Grove\n",
      "Jenna_McDougall\n",
      "Marty_Feier\n",
      "Lost_Dogs_and_Mixed_Blessings\n",
      "Bavarian_Fruit_Bread\n",
      "HoboSapiens\n",
      "The_Scooby-Doo_Show\n",
      "Avinash_Sachdev\n",
      "Connor_Byers\n",
      "Dorothy_Collins\n",
      "Darije_Kalezić\n",
      "The_Mummy's_Shroud\n",
      "The_Best_of_Earth,_Wind_&_Fire,_Vol._1\n",
      "Frank_Butler_(musician)\n",
      "Joey_Newman\n",
      "Buena_Familia\n",
      "Fantastic_Max\n",
      "Saxobeats\n",
      "Rangrasiya\n",
      "Make_Me_Smile_(Come_Up_and_See_Me)\n",
      "The_Pac-Man/Little_Rascals/Richie_Rich_Show\n",
      "T.A.T.u._Remixes\n",
      "Nick_Robinson\n",
      "Warren_Hill_(musician)\n",
      "The_Mumbly_Cartoon_Show\n",
      "Jagjit_Singh\n",
      "Isadar\n",
      "Ford_Star_Jubilee\n",
      "John_Berry_(singer)\n",
      "Defying_Gravity_(TV_series)\n",
      "Timo_Pärssinen\n",
      "The_Formula_(1980_film)\n",
      "Aya_(Japanese_singer)\n",
      "The_Curse_of_the_Werewolf\n",
      "Moon_Landing_(album)\n",
      "Tomas_Haake\n",
      "Jazz_Concerto_Grosso\n",
      "The_New_Shmoo\n",
      "Doin'_It_Big_(Young_JV_album)\n",
      "Leonid_Agutin\n",
      "Jerome_Benton\n",
      "Rusty_Day\n",
      "Mikko_Sirén\n",
      "It's_Showtime_(variety_show)\n",
      "Teddy_Charles\n",
      "Call_Me_(Petula_Clark_song)\n",
      "Stephen_Kozmeniuk\n",
      "Lisa_Henson\n",
      "Jerry_Peters\n",
      "Lucas_Silveira\n",
      "Robin_Pecknold\n",
      "Rat_Skates\n",
      "I_Think_I_See_Myself_on_CCTV\n",
      "Joey_Fatone\n",
      "Bosco_Mann\n",
      "Tony_Laureano\n",
      "Yeh_Vaada_Raha_(TV_series)\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "(Rites_of_Passage_(Indigo_Girls_album), subsequentWork, ?)\n",
      "\n",
      "In addition to the incomplete triple, an explanation is provided.\n",
      "An explanation is a set of RDF triples that provide context for the prediction.\n",
      "Given an incomplete triple (s, p, ?), all the RDF triples in the explanation feature s as subject or as object and connect it with another entity.\n",
      "Explanation:\n",
      "\n",
      "(Swamp_Ophelia, previousWork, Rites_of_Passage_(Indigo_Girls_album))\n",
      "\n",
      "This explanation is also provided in the form of quotient triples to provide a more abstract view of the explanation.\n",
      "A quotient triple is an RDF triple featuring the subject entity of the incomplete triple as subject or as object and connects it with a concept name rather than an entity.\n",
      "Considering an example explanation with the following triples:\n",
      "(s, p, e)\n",
      "(s, p, f)\n",
      "(s, p, g)\n",
      "\n",
      "The corresponding quotient triples are:\n",
      "(s, p, C1)\n",
      "(s, p, C2)\n",
      "\n",
      "where C1, C2 are concept names, e and f are instances of C1, and g is an instance of C2.\n",
      "\n",
      "Your answer still must be a single entity name even if the quotient triples contain concept names.\n",
      "\n",
      "Finally, the quotient triples are:\n",
      "\n",
      "(Rites_of_Passage_(Indigo_Girls_album), previousWork, [Album, MusicAlbum, Q482994])\n",
      "Swamp_Ophelia is an instance of Album, MusicAlbum, Q482994\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_template.invoke(explained_queries[0]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_exp_simulations = chain.batch(explained_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Swamp_Ophelia',\n",
       " 'Cutfather',\n",
       " 'Plant',\n",
       " 'Plant',\n",
       " 'Plant',\n",
       " 'Mushroom_Records',\n",
       " 'Moscow,_Idaho',\n",
       " 'Fleetwood_Mac',\n",
       " 'Vatican_City',\n",
       " 'Electronic',\n",
       " 'Orchidaceae',\n",
       " 'Universal_Studios',\n",
       " 'Tucson,_Arizona',\n",
       " 'Karen_Clark_Sheard',\n",
       " 'Airbus_Military',\n",
       " 'New_Iberia,_Louisiana',\n",
       " 'Bun_B',\n",
       " 'South_Korea',\n",
       " 'Cicadinae',\n",
       " 'Michael_Szloszer',\n",
       " 'Megaloptera',\n",
       " 'Plant',\n",
       " 'Plant',\n",
       " 'Mind_on_the_Moon',\n",
       " 'Sharon_Lowe',\n",
       " 'Q19088',\n",
       " 'Bee',\n",
       " 'Robyn_Is_Here',\n",
       " 'Thriller_(genre)',\n",
       " 'Uganda',\n",
       " 'Miki_Garrod',\n",
       " 'Single',\n",
       " 'Zavalaz',\n",
       " 'Vancouver',\n",
       " 'Allopterigeron',\n",
       " 'Persim_Maros',\n",
       " 'Mindy_McCready',\n",
       " 'Australia',\n",
       " 'London',\n",
       " 'Q729',\n",
       " 'K-pop',\n",
       " 'Central_Coast_AVA',\n",
       " 'Phnom_Penh_Crown_FC',\n",
       " 'Augustus_George_Vernon_Harcourt',\n",
       " 'Liu_Shaoqi',\n",
       " 'Aston_Martin_V8',\n",
       " 'Moth',\n",
       " 'Plantae',\n",
       " 'Iran',\n",
       " 'Abiy_Ahmed',\n",
       " 'Germany',\n",
       " 'Plant',\n",
       " 'Warrant_(American_band)',\n",
       " 'Kenneth_Bulmer',\n",
       " 'Q134556',\n",
       " 'Windsor,_Connecticut',\n",
       " 'Rufus_(band)',\n",
       " 'Steven_Brill_(scriptwriter)',\n",
       " 'Târnava_Mică_River',\n",
       " 'Stalino',\n",
       " 'Arctiinae',\n",
       " \"L'Arc-en-Ciel\",\n",
       " 'Minsk',\n",
       " 'South_Korea',\n",
       " 'Spartan_South_Midlands_Football_League',\n",
       " 'Lusitano_G.C.',\n",
       " 'Single',\n",
       " 'Bretton_Byrd',\n",
       " 'Mumbai',\n",
       " 'Kentucky_Derby',\n",
       " 'Hip_hop',\n",
       " 'Drum_kit',\n",
       " 'Mumbai',\n",
       " 'Capitol_Records',\n",
       " 'Bangerz',\n",
       " 'Plant',\n",
       " 'Orchidaceae',\n",
       " 'Zee_Entertainment_Enterprises',\n",
       " 'Soviet_Union',\n",
       " 'Kono_Machi',\n",
       " \"Where'd_You_Go_(Fort_Minor_song)\",\n",
       " 'Ochakiv',\n",
       " 'Romanian_Socialist_Democratic_Party',\n",
       " 'FC_Asmaral_Moscow',\n",
       " 'Don_Omar_Presents:_Meet_the_Orphans',\n",
       " 'Lady_Antebellum',\n",
       " 'Lepidoptera',\n",
       " 'London',\n",
       " 'Ross_the_Boss',\n",
       " 'Kuyavian-Pomeranian_Voivodeship',\n",
       " 'Melanie_C_song',\n",
       " 'Noctuoidea',\n",
       " 'Epidendroideae',\n",
       " 'Göttingen',\n",
       " 'Q4830453',\n",
       " 'Weezer',\n",
       " 'Angiosperm',\n",
       " 'Lebanon',\n",
       " 'Percussion_instrument',\n",
       " 'Sky_(band)']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_exp_simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictability_pre = [1 if o == gt else 0 for o, gt in zip(simulations, gts)]\n",
    "predictability_post = [1 if o == gt else 0 for o, gt in zip(post_exp_simulations, gts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_labels = [post - pre for post, pre in zip(predictability_post, predictability_pre)]\n",
    "gt_explanation_labels = [x[\"label\"] for x in explained_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, cohen_kappa_score\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_explanation_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplanation_labels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(gt_explanation_labels, explanation_labels, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(cohen_kappa_score(gt_explanation_labels, explanation_labels))\n",
      "File \u001b[0;32m~/envs/dixti/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/dixti/lib/python3.11/site-packages/sklearn/metrics/_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    248\u001b[0m     {\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/envs/dixti/lib/python3.11/site-packages/sklearn/metrics/_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    114\u001b[0m             type_true, type_pred\n\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "print(confusion_matrix(gt_explanation_labels, explanation_labels))\n",
    "print(classification_report(gt_explanation_labels, explanation_labels, zero_division=0))\n",
    "print(cohen_kappa_score(gt_explanation_labels, explanation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28 - 0.04 = 0.24000000000000002\n"
     ]
    }
   ],
   "source": [
    "avg_pre = sum(predictability_pre) / len(predictability_pre)\n",
    "avg_post = sum(predictability_post) / len(predictability_post)\n",
    "\n",
    "print(f\"{avg_post} - {avg_pre} = {avg_post - avg_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "\n",
    "# correlation, p_value = spearmanr(gt_explanation_labels, explanation_labels)\n",
    "\n",
    "# print(f\"Spearman Rank Correlation: {correlation}\")\n",
    "# print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = [i for i, ep in enumerate(explained_preds) if ep[\"label\"] == 1]\n",
    "\n",
    "# print(idxs)\n",
    "\n",
    "# for i in idxs:\n",
    "#     print(f\"{predictability_post[i]} - {predictability_pre[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = []\n",
    "# for i, explained_pred in enumerate(explained_preds):\n",
    "#     output_ = {\n",
    "#         \"s\": explained_pred[\"pred\"][0],\n",
    "#         \"p\": explained_pred[\"pred\"][1],\n",
    "#         \"o\": explained_pred[\"pred\"][2],\n",
    "#         \"simulation\": simulations[i],\n",
    "#         \"post_exp_simulation\": post_exp_simulations[i],\n",
    "#         \"explanation\": explained_pred[\"explanation\"],\n",
    "#     } \n",
    "\n",
    "#     output.append(output_)\n",
    "\n",
    "# df = pd.DataFrame(output, index=None)\n",
    "# df.to_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   method mode      model  dataset summarization\n",
    "# 1 kelpie necessary TransE DB50K   no\n",
    "#\n",
    "# 1 plain                             \n",
    "# 2 plain + ranking                   \n",
    "# 3 plain + esempi                    \n",
    "# 4 plain + ranking + esempi          \n",
    "#\n",
    "# 2 kelpie necessary TransE DB100K  no\n",
    "#\n",
    "# 1 plain                             \n",
    "# 2 plain + ranking                   \n",
    "# 3 plain + esempi                    \n",
    "# 4 plain + ranking + esempi          \n",
    "#\n",
    "# 3 kelpie necessary TransE DB100K  simulation\n",
    "#\n",
    "# 1 plain                             \n",
    "# 2 plain + ranking                   \n",
    "# 3 plain + esempi           \n",
    "# 4 plain + ranking + esempi   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
